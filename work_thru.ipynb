{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# This is my working topics covered\n",
    "\n",
    "## Data prep\n",
    "- before doing anything, we need to examine the quality of the data. always remember: \"garbage in, garbage out\". can we trust this data? does it come from a quality source? don't be misled by missing values. good data sources can be both verifiably accurate and also incomplete\n",
    "- since this data was collected from the us census bureau, there really isn't a higher authority on these statistics, so we have to trust that they were compiled coherently and comprehensively\n",
    "- once the data has been ingested and examined as is, we can start to see flaws in the data that need to be amended before it can be handed over to a machine learning algorithm\n",
    "- the columns, or \"features\" as data scientists usually call them, are unlabeled. we can find those labels in the metadata at the bottom of the document, where the unique values for each feature are listed. we can use python to easily parse that text data and grab just the feature names, which we can apply to the data\n",
    "- now that we can easily describe our features, we can begin doing what's called \"feature engineering\" on this dataset. feature engineering is basically the task of a) making our features either more machine-friendly or b) drawing information from features to create new features.\n",
    "- in the case of making the features more machine-friendly, let's look at the feature 'education'. we know as humans that earning a higher level of degree, like a master's or phd, will increase the probability that you make more money. a computer isn't a human and only understands 0's and 1's. thus, it's useful to turn those types of categorical variables into a number of some sort, which is a process called encoding. there are lots of possible encoding schemes, the most popular of which are label encoding and one-hot encoding. each of these has their own strengths and weaknesses, which we can discuss #######TODO target encoding??\n",
    "- in the case of drawing information from new features, let's look at the features 'capital gain' and 'capital loss'. these are very good candidates to reduce from two features into one. think about it intuitively, a unique individual cannot have both a capital gain AND a capital loss in a given year, so we can simply create a single 'capital_change' feature\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}